{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eff11c69-b5a1-4f5d-8fe5-a95874d3cbad",
   "metadata": {
    "id": "eff11c69-b5a1-4f5d-8fe5-a95874d3cbad"
   },
   "source": [
    "# Machine Learning Interpretability with Shapley values\n",
    "\n",
    "Shapley values give the contribution of each feature in an example of interest to the model prediction.\n",
    "\n",
    "Other recources:\n",
    "\n",
    "I thoroughly recommend this YouTube video from 'Machine Learning Dojo with Tim Scarfe':\n",
    "https://youtu.be/jhopjN08lTM\n",
    "\n",
    "Christopher Molner has an excellent online book with details on Shapley values:\n",
    "https://christophm.github.io/interpretable-ml-book/shapley.html\n",
    "\n",
    "There is also a good interview with Chris Molner on YouTube in machine Learning Street Talk: https://youtu.be/0LIACHcxpHU\n",
    "\n",
    "Documentation for the SHAP library: https://shap.readthedocs.io/en/latest/index.html\n",
    "\n",
    "Talk on the SHAP library by Scott Lunberg, a key author of the SHAP library: https://youtu.be/B-c8tIgchu0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbb9990-9823-4155-a495-0f77ca17892c",
   "metadata": {
    "id": "5cbb9990-9823-4155-a495-0f77ca17892c"
   },
   "source": [
    "## First set up Google Colab\n",
    "\n",
    "First ensure you have selected GPU from 'runtime' / 'change runtime type'. This may require you then to restart the runtime.\n",
    "\n",
    "Then run the code below to change TensorFlow version to 2.2.0. \n",
    "\n",
    "AFTER RUNNING THIS CELL PLEASE RESTART THE RUNTIME."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a305554e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a305554e",
    "outputId": "8c9a8da4-7004-43bd-8181-1d230ba3448f"
   },
   "outputs": [],
   "source": [
    "# Select 'GPU' from 'runtime' / 'change runtime type'\n",
    "\n",
    "downgrade_tensorflow = True\n",
    "\n",
    "import tensorflow\n",
    "print(tensorflow.__version__)\n",
    "\n",
    "if downgrade_tensorflow and tensorflow.__version__ != '2.2.0':\n",
    "    !pip uninstall tensorflow\n",
    "    !pip install tensorflow==2.2.0\n",
    "    import tensorflow\n",
    "    print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d0e2ce",
   "metadata": {
    "id": "d3d0e2ce"
   },
   "source": [
    "## Shapley Values - Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0edf20b",
   "metadata": {
    "id": "a0edf20b"
   },
   "source": [
    "### Shapley values in game theory\n",
    "\n",
    "The origin of Shapley values is in game theory, and is used to provide a fair determination of the value each player makes in a coalition [1].\n",
    "\n",
    "Imagine we have three players who play together in a coalition in a pub quiz. Together they win £100 on average. How should their winnings be split to fairly compensate each person for contribution to the collective winnings?\n",
    "\n",
    "Shapley values is one way to get at this, and goes like this. Imagine the coalition being formed one quiz player at a time, and the value of the player is the difference in winnings by adding in that one player.\n",
    "\n",
    "So player A alone may win £75, and so in this case adds a value of £75. Player A+B together win £90 and so player B has added £15 of value vs. player A alone. Players ABC together win £100, so player C has added £10 of value on top of player A.\n",
    "\n",
    "But - some questions are easier and all may know the answer. If we only have one order of addition then the first player (who plays alone) will seem to have the hightest value. Player B can only add value to those questions that player A does not know the answer to, and player C can only add value to those questions that players A and B together do not know the answer to.\n",
    "\n",
    "So a full Shapley analysis has all permutations, allowing all players to be first, and second to other players, and third. The table below shows all the permutations. Those in italics have been performed before and would not need repeating (player combination A+B is the same as player combination B+A).\n",
    "\n",
    "Permutation| Sequence to test\n",
    "---|---\n",
    "ABC | A, AB, ABC\n",
    "ACB | *A*, AC, *ACB*\n",
    "BAC | B, *BA*, *BAC*\n",
    "BCA | *B*, BC, *BCA*\n",
    "CAB | C, CA, *CAB*\n",
    "CBA | *C*, *BA*, *CBA*\n",
    "\n",
    "For each combination tested (including single players) we calulcate the value they added in the step-wise addition of players. We then average the added value each player makes in different combinations. That average gives us the average marginal contribution each player makes. The sum of these average marginal contribution will add up to average winnings of the whole team playing together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfac6e46",
   "metadata": {
    "id": "bfac6e46"
   },
   "source": [
    "### Shapley values in machine learning\n",
    "\n",
    "So what is the link to machine learning?\n",
    "\n",
    "Shapley values provide a measure of contribution made by different features to a prediction.\n",
    "\n",
    "Imagine I have three features for prediction of house price: size, number of bedrooms, area crime rate. I am interested in knowing, using any type of machine learning model, the contribution each feature made to a prediction.\n",
    "\n",
    "In Shapley values in machine learning the total contribution features make to a prediction is the difference in prediction for that particular instance compared with the mean value for the whole population. In the case of house prices, if I am given no feature information then all my predictions will just be the average price. As I add in features (in different combinations) I get closer to the prediction using all information (features).\n",
    "\n",
    "Here we are likely to have covariance between features - that is they are linked. The size of a property is likely to have some relationship with the number of rooms. And perhaps larger houses are also built in lower crime areas. So again the first feature added will probably grab much of the total prediction value, just like the first pub quiz player grabbed much of the total winnings. So, as with my pub quiz team I can run all combinations of adding features, fitting the model to each combination and calculating the value added (difference between that prediction and just the average house price) by different features. I then average the value added for each feature across all permutations tested."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb118c4-4de5-4bd1-9a69-499c58de1e3b",
   "metadata": {
    "id": "5bb118c4-4de5-4bd1-9a69-499c58de1e3b"
   },
   "source": [
    "### Four key properties of Shapley properties\n",
    "\n",
    "Shapley values obey four key properties of explainability:\n",
    "\n",
    "* *Efficiency*: The feature contributions add up to the difference in the prediction for x from the average.\n",
    "\n",
    "* *Symmetry*: The contributions of two features values are the same if they contribute equaly to all possible combinations of features used. This is also called *equal treatment of equals*\n",
    "\n",
    "* *Additivity* or *Linearity*: The prediction in any case should be proportional to the Shapley values for that case. Suppose you trained a random forest, which means that the prediction is an average of many decision trees. The Additivity property guarantees that for a feature value, you can calculate the Shapley value for each tree individually, average them, and get the Shapley value for the feature value for the random forest.\n",
    "\n",
    "* *Nullity* or *Dummy*: If a feature does not improve preidction then the Shapley value for that feature should be zero.If a feature does not improve preidction then the Shapley value for that feature should be zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cad2174",
   "metadata": {
    "id": "0cad2174"
   },
   "source": [
    "## Shapley values in practice\n",
    "\n",
    "The above gives the intuition behind Shapley values, but it is rare that we can do this in practice. As the number fo features grow, the number of permutations explode - giving us an unfeasible number of models to fit and test.\n",
    "\n",
    "So, two methods can be used (and these days, the second method, the SHAP library, is almost universally used).\n",
    "\n",
    "### Shapley sampling\n",
    "\n",
    "In Shapley sampling we use a couple of tricks to make the problem more computationally acceptable:\n",
    "\n",
    "1) We do not refit models excluding features, but rather we approximate the model fit without a feature, by replacing that feature value either with the mean or median value of that feature across all values, or by replacing that feature value with a random sample from the population (and we are likely to repeat this to get a number of estimates).\n",
    "\n",
    "2) We sample from possible permutaions and take the mean. As the number of samples increases the mean sampled Shapely estimate will tend towards the true Shapley value.\n",
    "\n",
    "Strumbelj et al. [2] provided the following method, which tests the value of adding feature *j* in combination with any other feature values:\n",
    "\n",
    "To estimate for Shapley values for any instance of interest, *x*:\n",
    "\n",
    "Repeat for each feature, *j*:\n",
    "\n",
    "  * Repeat the following for *m* iterations:\n",
    "  \n",
    "    * Draw a random instance, *z* from the population\n",
    "\n",
    "    * Choose a random order of feature values\n",
    "\n",
    "    * Create two hybrid instances. Both are identical up to, and after, feature *j*, but one instance takes the value of *j* from our instance of interest *x*, and the other from our random sample, *z*:\n",
    "      \n",
    "      1. Use values from *x* for all feature values up to *and including* feature *j*, and then use values from *z* for the remaining feature values.\n",
    "      2. Use values from *x* for all feature values up to *but not including* feature *j*, and then use values from *z* for the remaining feature values, including feature *j*.\n",
    "     \n",
    "     * Compute the marginal contribution of feature *j* by taking model estimates of hybrid instances with and without *j* from *x*: *f(x+j)* - *f(x-j*)\n",
    "      \n",
    "      * Computer the Shapley value for *j* in instance *x* as the average of the m* iterations\n",
    "\n",
    "### SHAP \n",
    "\n",
    "SHAP is a python Library with efficient algorithms for estimatign Shapley values. For more details see:\n",
    "\n",
    "https://shap.readthedocs.io/en/latest/\n",
    "\n",
    "https://github.com/slundberg/shap\n",
    "\n",
    "Lundberg, A. and Lee, S-I (2017) A Unified Approach to Interpreting Model Predictions. arXiv:1705.07874 https://arxiv.org/abs/1705.07874\n",
    "\n",
    "\n",
    "Here, we will use the SHAP library,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e47f48",
   "metadata": {
    "id": "52e47f48"
   },
   "source": [
    "### References\n",
    "\n",
    "[1]  Shapley, Lloyd S. (August 21, 1951). \"Notes on the n-Person Game -- II: The Value of an n-Person Game\" (PDF). Santa Monica, Calif.: RAND Corporation. https://www.rand.org/content/dam/rand/pubs/research_memoranda/2008/RM670.pdf\n",
    "\n",
    "[2] Štrumbelj, Erik, and Igor Kononenko. \"Explaining prediction models and individual predictions with feature contributions.\" Knowledge and information systems 41.3 (2014): 647-665"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1CyhqbOXF7a4",
   "metadata": {
    "id": "1CyhqbOXF7a4"
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d48e3a",
   "metadata": {
    "id": "30d48e3a"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Install shap if necessary (e.g. running in Google Colab)\n",
    "try:\n",
    "    import shap\n",
    "except:\n",
    "    !pip install shap\n",
    "    import shap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3930f893",
   "metadata": {
    "id": "3930f893"
   },
   "source": [
    "## Regression Analysis: Boston House Price Predictions\n",
    "\n",
    "The Boston House Price data set provides various features descsribing a house, and its value. It is commonly used to demonstrate multiple regression.\n",
    "\n",
    "The data is available in `shap.datasets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5d88ab",
   "metadata": {
    "id": "0b5d88ab"
   },
   "outputs": [],
   "source": [
    "X, y = shap.datasets.boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff5ff99",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "dff5ff99",
    "outputId": "2249dc83-2074-4f18-a6f1-264cc21da875"
   },
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fecbf67",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7fecbf67",
    "outputId": "50f807d2-a6bc-4922-b45b-b72887fd3361"
   },
   "outputs": [],
   "source": [
    "# Show value to bepredicted (y, house price)\n",
    "y[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbc18f8",
   "metadata": {
    "id": "9fbc18f8"
   },
   "source": [
    "### Split training and test\n",
    "\n",
    "We use SciKit's Learn's method for spliting data into training (80%) and test (20%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f0f701",
   "metadata": {
    "id": "f9f0f701"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfd6193",
   "metadata": {
    "id": "8bfd6193"
   },
   "source": [
    "### Fit Linear Regression Model\n",
    "\n",
    "We use SciKit Learn's linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc50a72",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cdc50a72",
    "outputId": "3a587e90-b3a4-4a7a-8b55-1debdc42ff2c"
   },
   "outputs": [],
   "source": [
    "# Create a Linear regressor\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train the model using the training sets \n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0301599",
   "metadata": {
    "id": "b0301599"
   },
   "outputs": [],
   "source": [
    "# Model prediction on train data\n",
    "y_pred = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35be2ff",
   "metadata": {
    "id": "e35be2ff"
   },
   "source": [
    "### Make predictions and test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907d543d",
   "metadata": {
    "id": "907d543d"
   },
   "outputs": [],
   "source": [
    "# Model prediction on train data\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac69e06",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dac69e06",
    "outputId": "cf099036-2cc3-4feb-f3c3-970bb9d77446"
   },
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "print(f'R^2: {metrics.r2_score(y_test, y_pred):0.3f}')\n",
    "print(f'MAE: {metrics.mean_absolute_error(y_test, y_pred):0.1f}')\n",
    "print(f'MSE: {metrics.mean_squared_error(y_test, y_pred):0.1f}')\n",
    "print(f'RMSE:{np.sqrt(metrics.mean_squared_error(y_test, y_pred)):0.1f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e47a9ab",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "2e47a9ab",
    "outputId": "ef0f7b90-e7fa-4a71-f01a-19337ee30aef"
   },
   "outputs": [],
   "source": [
    "# Visualizing the differences between actual prices and predicted values\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.xlabel(\"Actural price\")\n",
    "plt.ylabel(\"Predicted price\")\n",
    "plt.title(\"Predicted Prices vs Actual Prices\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef5725e",
   "metadata": {
    "id": "8ef5725e"
   },
   "source": [
    "### Getting Shapley values with SHAP\n",
    "\n",
    "We use the `explainer` and `shap_values` methods from the SHAP library to get Shapley values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smfb8j4WIwqQ",
   "metadata": {
    "id": "smfb8j4WIwqQ"
   },
   "source": [
    "### Global influence of features in the training set\n",
    "\n",
    "Here we are lookign at Shapley values for the training set, to understand the influence of features in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13aee6f0",
   "metadata": {
    "id": "13aee6f0"
   },
   "outputs": [],
   "source": [
    "# Train explainer on Training set\n",
    "explainer = shap.Explainer(model, X_train)\n",
    "# Get Shapley values\n",
    "shap_values = explainer(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Vd87a6aIIUDT",
   "metadata": {
    "id": "Vd87a6aIIUDT"
   },
   "source": [
    "The `shap.summary_plot` shows the global influence of features - that is the average influence of features across the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0807b9f7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 429
    },
    "id": "0807b9f7",
    "outputId": "5d0334c4-c2a7-458a-c6fe-e124c5013eba"
   },
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_test, plot_type='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EGkTedd5I3y3",
   "metadata": {
    "id": "EGkTedd5I3y3"
   },
   "source": [
    "### Influence of features in individual examples in the test set\n",
    "\n",
    "Here we will look at individial instance predictions, and how the different features contributed to prediction for that instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rBRIQidiJT5T",
   "metadata": {
    "id": "rBRIQidiJT5T"
   },
   "outputs": [],
   "source": [
    "# Train explainer on Training set\n",
    "explainer = shap.Explainer(model, X_test)\n",
    "# Get Shapley values\n",
    "shap_values = explainer(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TwJaJAyqJZwn",
   "metadata": {
    "id": "TwJaJAyqJZwn"
   },
   "source": [
    "Show a `waterfall` plot for the first instance in the test set. The `waterfall` plot starts with the average house price, *E[f(X)]*, at the bottom, and shows the influence of each feature, ending with the most influential feature at the top along with the predicted house price, *f(x)*.\n",
    "\n",
    "Shap values are coloured: red bars show features that increase the predicted price compared with the average house price, and blue bars show features that reduce the predicted price compared with the average house price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6adc9c3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 525
    },
    "id": "e6adc9c3",
    "outputId": "7986f679-ca12-49ac-9f32-7a1a0584711f"
   },
   "outputs": [],
   "source": [
    "shap.plots.waterfall(shap_values[0], max_display=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nUgAQFZFKSD2",
   "metadata": {
    "id": "nUgAQFZFKSD2"
   },
   "source": [
    "The `beeswarm` plot displays the Shapley values for all instances in the test set. Shap values are coloured by the effect they have "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39cd68d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "id": "c39cd68d",
    "outputId": "30982d94-15c7-49e1-c050-078759b9cb6d"
   },
   "outputs": [],
   "source": [
    "shap.plots.beeswarm(shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MEmr05MZKRBe",
   "metadata": {
    "id": "MEmr05MZKRBe"
   },
   "source": [
    "The `force` plot is an alternative to the `waterfall` plot, showing the influence of each feature in a single instance. \n",
    "\n",
    "This plot uses javascript, and so `shap.initjs` is needed to be tun once in the notebook. Unlike other plots this plot is not displayed when a notebook is re-opened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55968ac8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 193
    },
    "id": "55968ac8",
    "outputId": "f250fb21-78d2-412e-84bc-77e05e6bb2c7"
   },
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "shap.force_plot(shap_values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b82ba1f",
   "metadata": {
    "id": "4b82ba1f"
   },
   "source": [
    "## Classification: Titanic Survival\n",
    "\n",
    "Here we load upa data set giving feature values for passengers on the Titanic, and whether they survived or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e62a14b",
   "metadata": {
    "id": "6e62a14b"
   },
   "outputs": [],
   "source": [
    "# Import machine learning methods\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e5ba95",
   "metadata": {
    "id": "a0e5ba95"
   },
   "outputs": [],
   "source": [
    "# Data will be loaded from web if not available locally (for Google Colab)\n",
    "try:\n",
    "    data = pd.read_csv('data/titanic.csv')\n",
    "except:\n",
    "    # Download processed data:\n",
    "    address = 'https://raw.githubusercontent.com/MichaelAllen1966/' + \\\n",
    "                '1804_python_healthcare/master/titanic/data/processed_data.csv'\n",
    "    \n",
    "    data = pd.read_csv(address)\n",
    "\n",
    "# Make all data 'float' type\n",
    "data = data.astype(float)\n",
    "# Drop passenger ID\n",
    "data.drop(['PassengerId'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6778bcb2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "6778bcb2",
    "outputId": "022809e6-b0f9-4447-dab7-f1d2bc766e55"
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8SHXLJH9L80x",
   "metadata": {
    "id": "8SHXLJH9L80x"
   },
   "source": [
    "Split X, and y, and split into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae80ef0",
   "metadata": {
    "id": "2ae80ef0"
   },
   "outputs": [],
   "source": [
    "X = data.drop(['Survived'],axis=1) # X = all 'data' except the 'survived' column\n",
    "y = data['Survived'] # y = 'survived' column from 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354305e8",
   "metadata": {
    "id": "354305e8"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "N6kqNZxtMFIk",
   "metadata": {
    "id": "N6kqNZxtMFIk"
   },
   "source": [
    "Define a function to standardise the data using SciKit Learn's `StandardScalar`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7450e2",
   "metadata": {
    "id": "dc7450e2"
   },
   "outputs": [],
   "source": [
    "def standardise_data(X_train, X_test):\n",
    "    \n",
    "    # Initialise a new scaling object for normalising input data\n",
    "    sc = StandardScaler() \n",
    "\n",
    "    # Set up the scaler just on the training set\n",
    "    sc.fit(X_train)\n",
    "\n",
    "    # Apply the scaler to the training and test sets\n",
    "    train_std=sc.transform(X_train)\n",
    "    test_std=sc.transform(X_test)\n",
    "    \n",
    "    # Convert back to DataFrame\n",
    "    train_std = pd.DataFrame(train_std, columns=list(X_train))\n",
    "    test_std = pd.DataFrame(test_std, columns=list(X_train))\n",
    "    \n",
    "    return train_std, test_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f95cb6",
   "metadata": {
    "id": "51f95cb6"
   },
   "outputs": [],
   "source": [
    "X_train_std, X_test_std = standardise_data(X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NWC5fmOlMWqV",
   "metadata": {
    "id": "NWC5fmOlMWqV"
   },
   "source": [
    "Fit logistic regression model on stnadradised data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957d2c84",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "957d2c84",
    "outputId": "868ceddb-5271-4107-8eb9-344d03920e78"
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train_std, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ll4f9sSmMfNw",
   "metadata": {
    "id": "ll4f9sSmMfNw"
   },
   "source": [
    "Get training Shapley values for the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4e3609",
   "metadata": {
    "id": "1b4e3609"
   },
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(model, X_train_std)\n",
    "shap_values = explainer(X_train_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "N-CX8YDzMmLg",
   "metadata": {
    "id": "N-CX8YDzMmLg"
   },
   "source": [
    "Show global infleunce of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161d72b1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 581
    },
    "id": "161d72b1",
    "outputId": "1fd2886b-91a7-421d-db0d-054816b59850"
   },
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_test_std, plot_type='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HPyHfzLKMttO",
   "metadata": {
    "id": "HPyHfzLKMttO"
   },
   "source": [
    "Get Shapley values for test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hCqfNaS0MxpA",
   "metadata": {
    "id": "hCqfNaS0MxpA"
   },
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(model, X_test_std)\n",
    "shap_values = explainer(X_test_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "V0-sSivFM20_",
   "metadata": {
    "id": "V0-sSivFM20_"
   },
   "source": [
    "Show `beeswarm` plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7456640",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "id": "f7456640",
    "outputId": "b9e3ca4b-61ed-47a6-e6f4-059397756eb9"
   },
   "outputs": [],
   "source": [
    "shap.plots.beeswarm(shap_values, max_display=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZrZ2GeUTNcAf",
   "metadata": {
    "id": "ZrZ2GeUTNcAf"
   },
   "source": [
    "Let's examine the first test set instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c02ef5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "93c02ef5",
    "outputId": "94415a86-fdf4-49ad-f2cb-1ad5f5f15dce"
   },
   "outputs": [],
   "source": [
    "X_test.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gk25tqSFNjEf",
   "metadata": {
    "id": "gk25tqSFNjEf"
   },
   "source": [
    "We'll show the `waterfall` plot. Note that the output is the influence on the log odds probability of survival. This is displayred rather than probability of survival as log odds are additive, whereas probability is not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c66b2c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "id": "a9c66b2c",
    "outputId": "a15d8bb4-f718-47ad-df5d-83d963850356"
   },
   "outputs": [],
   "source": [
    "# Output is log odds of survival (not probability)\n",
    "shap.plots.waterfall(shap_values[0], max_display=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d772bde",
   "metadata": {
    "id": "7d772bde"
   },
   "source": [
    "(I Haven't yet got a foce plot to plot proplery for classification!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2439b4d9",
   "metadata": {
    "id": "2439b4d9"
   },
   "source": [
    "## Image classification using the MNIST hand-written image set\n",
    "\n",
    "Here we will get Shapley values for pixels in images, using Convolutional Neural Net classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6e4b55",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9b6e4b55",
    "outputId": "b0e5ab04-3d78-4f92-df58-d6a388892101"
   },
   "outputs": [],
   "source": [
    "# This is the code from:\n",
    "# https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# Get mnist data set from keras datasets\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "batch_size = 32\n",
    "num_classes = 10\n",
    "epochs = 50\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# Set up CNN\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Fit model\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=2,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jp4xLt0LPIw3",
   "metadata": {
    "id": "jp4xLt0LPIw3"
   },
   "source": [
    "We need to set a background that individual instances will be compared against. We will select 250 images at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da16c17",
   "metadata": {
    "id": "0da16c17"
   },
   "outputs": [],
   "source": [
    "# select a set of background examples to take an expectation over\n",
    "background = x_train[np.random.choice(x_train.shape[0], 250, replace=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UHV6wiEDPg1G",
   "metadata": {
    "id": "UHV6wiEDPg1G"
   },
   "source": [
    "For TensforFlow neural nets we use SHAP's `DeepExplainer` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505a47ed",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "505a47ed",
    "outputId": "4a4ae6f3-fff0-44cd-99ed-d39c498a3641"
   },
   "outputs": [],
   "source": [
    "# explain predictions of the model on four images\n",
    "e = shap.DeepExplainer(model, background)\n",
    "# ...or we could pass tensors directly\n",
    "# e = shap.DeepExplainer((model.layers[0].input, model.layers[-1].output), background)\n",
    "\n",
    "# Get Shapley values\n",
    "shap_values = e.shap_values(x_test[1:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_uav65dmP306",
   "metadata": {
    "id": "_uav65dmP306"
   },
   "source": [
    "Show Shapley values in a plot, and here we demonstrate how to save a plot (which can be used on other plots apart from `force` plots)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f84713",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "id": "b2f84713",
    "outputId": "da3299d1-46d1-4182-b56a-579ffc8bf6a1"
   },
   "outputs": [],
   "source": [
    "# Get predictions of four examples\n",
    "predictions = model.predict(x_test).argmax(axis=1)\n",
    "print ('Shap values for ten possible clasees of four images')\n",
    "print ('Predictions:', list(predictions[1:5]))\n",
    "\n",
    "# Get Shapley value plot for five values\n",
    "shap.image_plot(shap_values, -x_test[1:5], show=False)\n",
    "# Put image in f\n",
    "f = plt.gcf()\n",
    "f.savefig('shap_image.jpg', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbad388e",
   "metadata": {
    "id": "fbad388e"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "shapley_colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
